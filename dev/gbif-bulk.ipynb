{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the dwca format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dwca.read import DwCAReader\n",
    "from dwca.darwincore.utils import qualname as qn\n",
    "\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from pathlib import Path\n",
    "\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from abc import abstractmethod\n",
    "\n",
    "from urllib.parse import unquote, urlparse\n",
    "from pathlib import PurePosixPath\n",
    "from PIL import Image\n",
    "\n",
    "import asyncio, asyncssh, sys\n",
    "from typing import TypedDict\n",
    "from collections import defaultdict\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occurrences_path = Path(\"/home/george/codes/gbif-request/data/classif/mini/0013397-241007104925546.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with DwCAReader(occurrences_path) as dwca:\n",
    "    print(dwca.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwca = DwCAReader(occurrences_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = dwca.get_corerow_by_position(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploration first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = []\n",
    "for row in dwca:\n",
    "    lengths+= [len(row.extensions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(lengths), max(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(lengths)\n",
    "plt.loglog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwca.extension_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwca.core_file.file_descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row.descriptor.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(d):\n",
    "    \"\"\"Pretty print for dictionary.\n",
    "    \"\"\"\n",
    "    d = {k.split('/')[-1]:v for k, v in d.items()}\n",
    "    print(json.dumps(d, indent=4))\n",
    "\n",
    "for i in range(len(row.extensions)):\n",
    "    pretty_print(row.extensions[i].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print(row.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(row.data.keys())), len(list(row.extensions[-1].data.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in row.extensions[-1].data.keys():\n",
    "    if k not in list(row.data.keys()):\n",
    "        print(k.split('/')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a downloader file : merge occurrence and multimedia metadata in a Python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_mult = [\n",
    "    # \"gbifID\",\n",
    "    \"type\",\n",
    "    \"format\",\n",
    "    \"identifier\",\n",
    "    \"references\",\n",
    "    \"created\",\n",
    "    \"creator\",\n",
    "    \"publisher\",\n",
    "    \"license\",\n",
    "    \"rightsHolder\"\n",
    "]\n",
    "\n",
    "keys_occ = [\n",
    "    \"gbifID\",\n",
    "\n",
    "    # Recording metadata\n",
    "    \"basisOfRecord\",\n",
    "    \"recordedBy\",\n",
    "    \"continent\",\n",
    "    \"countryCode\",\n",
    "    \"stateProvince\",\n",
    "    \"county\",\n",
    "    \"municipality\",\n",
    "    \"locality\",\n",
    "    \"verbatimLocality\",\n",
    "    \"decimalLatitude\",\n",
    "    \"decimalLongitude\",\n",
    "    \"coordinateUncertaintyInMeters\", \n",
    "    \"eventDate\",\n",
    "    \"eventTime\",\n",
    "\n",
    "    # Copyrights metadata\n",
    "    # \"license\",\n",
    "    # \"rightsHolder\",\n",
    "\n",
    "\n",
    "    # Individual metadata\n",
    "    \"sex\",\n",
    "\n",
    "    # Taxon metadata\n",
    "    \"acceptedNameUsageID\", \n",
    "    \"scientificName\", \n",
    "    \"kingdom\", \n",
    "    \"phylum\", \n",
    "    \"class\", \n",
    "    \"order\", \n",
    "    \"family\", \n",
    "    \"genus\",\n",
    "    \"specificEpithet\",\n",
    "    \"taxonRank\",\n",
    "    \"taxonomicStatus\",\n",
    "\n",
    "    # Storage metadata\n",
    "    \"taxonKey\",\n",
    "    \"acceptedTaxonKey\",\n",
    "    \"datasetKey\",\n",
    "    \"kingdomKey\",\n",
    "    \"phylumKey\",\n",
    "    \"classKey\",\n",
    "    \"orderKey\",\n",
    "    \"familyKey\",\n",
    "    \"genusKey\",\n",
    "    \"speciesKey\",\n",
    "    ]\n",
    "\n",
    "keys_file = [\n",
    "    \"filename\"\n",
    "]\n",
    "\n",
    "# Check if all the keys above are in the row metadata\n",
    "row_keys = [k.split('/')[-1] for k in list(row.data.keys())]\n",
    "for k in keys_occ:\n",
    "    if k not in row_keys:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row.extensions[0].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_metadata = {}\n",
    "\n",
    "# Add keys for occurrence and multimedia\n",
    "for k in keys_occ + keys_mult:\n",
    "    images_metadata[k] = []\n",
    "\n",
    "for row in dwca:\n",
    "\n",
    "    # The last element of the extensions is the verbatim and is (almost) a duplicate of row data\n",
    "    # And is thus not needed.\n",
    "    extensions = row.extensions[:-1]\n",
    "\n",
    "    for e in extensions:\n",
    "        # Do not consider empty URLs\n",
    "        identifier = e.data['http://purl.org/dc/terms/identifier']\n",
    "\n",
    "        if identifier != '':\n",
    "            # Add occurrence metadata\n",
    "            # This is identical for all multimedia\n",
    "            for k,v in row.data.items():\n",
    "                k = k.split('/')[-1]\n",
    "                if k in keys_occ:\n",
    "                    images_metadata[k] += [v]\n",
    "\n",
    "            # Add extension metadata\n",
    "            for k,v in e.data.items():\n",
    "                k = k.split('/')[-1]\n",
    "                if k in keys_mult:\n",
    "                    images_metadata[k] += [v]\n",
    "\n",
    "            # Add image name for future download\n",
    "            # Hashing of the image URL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the metadata\n",
    "\n",
    "# Solution 1 - with pandas\n",
    "output_path = occurrences_path.parent / \"tmp.parquet\"\n",
    "pd.DataFrame(images_metadata).to_parquet(output_path, engine='pyarrow', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove empty speciesKey and co."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(path = occurrences_path.parent / \"tmp.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBIF_KEYS = [\n",
    "    \"kingdomKey\",\n",
    "    \"phylumKey\",\n",
    "    \"classKey\",\n",
    "    \"orderKey\",\n",
    "    \"familyKey\",\n",
    "    \"genusKey\",\n",
    "    \"speciesKey\",\n",
    "]\n",
    "\n",
    "df = df.loc[df['speciesKey'].notna() & (df['speciesKey'] != '')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(path = occurrences_path.parent / \"tmp.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the duplicates from images_metadata\n",
    "# if a file is used several times all concerned rows are dropped.\n",
    "\n",
    "# Solution 1 - no pandas, maybe overly complicated\n",
    "\n",
    "# from collections import defaultdict\n",
    "\n",
    "# def list_duplicates(seq):\n",
    "#     \"\"\"https://stackoverflow.com/a/5419576\n",
    "#     \"\"\"\n",
    "#     tally = defaultdict(list)\n",
    "#     for i,item in enumerate(seq):\n",
    "#         tally[item].append(i)\n",
    "#     return ((key,locs) for key,locs in tally.items() \n",
    "#                             if len(locs)>1)\n",
    "\n",
    "# print(len(list(list_duplicates(images_metadata['identifier']))[0][1]))\n",
    "\n",
    "\n",
    "\n",
    "# Solution 2 - pandas, much simpler\n",
    "\n",
    "df = pd.DataFrame(images_metadata)\n",
    "# print(df.duplicated(subset='identifier', keep=False).astype(int).sum())\n",
    "df.drop_duplicates(subset='identifier', keep=False, inplace=True)\n",
    "print(df.duplicated(subset='identifier', keep=False).astype(int).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(output_path, engine='pyarrow', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limit number of download per species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(path = occurrences_path.parent / \"tmp.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_images_per_species = 500\n",
    "df = df.groupby('taxonKey').filter(lambda x: len(x) <= max_num_images_per_species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the scientific name of the maximum occurence\n",
    "# df.iloc[df.groupby('taxonKey').count().idxmax()]\n",
    "df[df['taxonKey'] == df['taxonKey'].value_counts().idxmax()]['scientificName'].iloc[0]\n",
    "# df[df['taxonKey'] == df.groupby('taxonKey')['taxonKey'].count().idxmax()]['scientificName'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(output_path, engine='pyarrow', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(path = occurrences_path.parent / \"tmp.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = df.identifier\n",
    "formats = df.format \n",
    "species = df.speciesKey\n",
    "occs = [(u,f,s) for u,f,s in zip(urls, formats, species)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occs = [(row.identifier, row.format, row.speciesKey) for row in df.itertuples(index=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the final df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occ_path = Path(\"/home/george/codes/gbifxdl/data/classif/mini/0013397-241007104925546.parquet\")\n",
    "df = pd.read_parquet(occ_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Postprocessing, remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "   'sha256': ['abc', 'abc', 'def', 'ghi', 'ghi', 'ghi'],\n",
    "   'speciesKey': [1, 1, 2, 3, 4, 3]\n",
    "})\n",
    "\n",
    "# Step 1: Group by sha256 and apply the heuristic\n",
    "def process_duplicates(group):\n",
    "    if group['speciesKey'].nunique() == 1:\n",
    "        # Only one speciesKey, keep one row\n",
    "        return group.iloc[:1]\n",
    "    else:\n",
    "        # Multiple speciesKey, drop all duplicates\n",
    "        return pd.DataFrame(columns=group.columns)\n",
    "\n",
    "# Apply the function to each group of sha256\n",
    "result = df.groupby('sha256', group_keys=False).apply(process_duplicates)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "   'sha256': ['abc', 'abc', 'def', 'ghi', 'ghi', 'ghi'],\n",
    "   'speciesKey': [1, 1, 2, 3, 4, 3],\n",
    "   'filename': ['file1.jpg', 'file2.jpg', 'file3.jpg', 'file4.jpg', 'file5.jpg', 'file6.jpg']\n",
    "})\n",
    "\n",
    "# List to store removed sha256 values and files\n",
    "removed_files = []\n",
    "\n",
    "# Function to process duplicates based on heuristic\n",
    "def process_duplicates(group):\n",
    "    if group['speciesKey'].nunique() == 1:\n",
    "        # Only one speciesKey, keep one row, delete the duplicates' files\n",
    "        for index, row in group.iloc[1:].iterrows():  # Keep the first row, delete the rest\n",
    "            file_path = f\"{row['speciesKey']}/{row['filename']}\"\n",
    "            # if os.path.exists(file_path):\n",
    "                # os.remove(file_path)\n",
    "            removed_files.append(file_path)\n",
    "        return group.iloc[:1]  # Keep only the first row\n",
    "    \n",
    "    else:\n",
    "        # Multiple speciesKey, remove all rows and delete associated files\n",
    "        for index, row in group.iterrows():\n",
    "            file_path = f\"{row['speciesKey']}/{row['filename']}\"\n",
    "            # if os.path.exists(file_path):\n",
    "                # os.remove(file_path)\n",
    "            removed_files.append(file_path)\n",
    "        \n",
    "        # Return an empty DataFrame for this group\n",
    "        return pd.DataFrame(columns=group.columns)\n",
    "\n",
    "# Apply the function to each group of sha256\n",
    "result = df.groupby('sha256', group_keys=False).apply(process_duplicates, include_groups=True)\n",
    "\n",
    "# Get the list of removed sha256 and files\n",
    "removed_files_list = removed_files\n",
    "\n",
    "# Output the cleaned DataFrame, removed sha256 list, and removed file paths\n",
    "print(\"Cleaned DataFrame:\")\n",
    "print(result)\n",
    "\n",
    "print(\"\\nList of removed files:\")\n",
    "print(removed_files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileManager:\n",
    "    @abstractmethod\n",
    "    def save(img, img_path):\n",
    "        pass \n",
    "\n",
    "    @abstractmethod\n",
    "    def remove(img_path):\n",
    "        pass \n",
    "\n",
    "class LocalFileManager(FileManager):\n",
    "    def save(img, img_path):\n",
    "        with open(img_path, 'wb') as handler:\n",
    "            handler.write(img)\n",
    "    \n",
    "    def remove(img_path):\n",
    "        if os.path.exists(img_path):\n",
    "            os.remove(img_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(path = occurrences_path.with_suffix('.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sha256'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sha256'] = np.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('/home/george/codes/gbifxdl/data/classif/mini/0013397-241007104925546.parquet', engine='pyarrow', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'sftp://erda:2222/datasets/tests'\n",
    "\n",
    "PurePosixPath(unquote(urlparse(url).path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(urlparse(url).path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_paths(folder):\n",
    "    \"\"\"\n",
    "    Recursively collect all image file paths in a folder.\n",
    "    Returns a list of image file paths.\n",
    "    \"\"\"\n",
    "    image_paths = []\n",
    "    for root, _, files in os.walk(folder):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')):\n",
    "                image_paths.append(os.path.join(root, file))\n",
    "    return image_paths\n",
    "\n",
    "files = get_image_paths(\"/home/george/codes/gbifxdl/data/classif/mini/images\")\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_dir = \"sftp://gmo@ecos.au.dk:@io.erda.au.dk/datasets/test3\"\n",
    "o = urlparse(remote_dir)\n",
    "remote_dir = Path(o.path)\n",
    "netloc = o.netloc\n",
    "sftp_server = f\"{o.scheme}://{o.netloc}\"\n",
    "print(sftp_server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u,s=o.netloc.split(':@')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check integrity of Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_parquet(\"/home/george/codes/gbifxdl/data/classif/mini/0013397-241007104925546.parquet\")\n",
    "df1 = pd.read_parquet(\"/home/george/codes/gbifxdl/data/classif/lepi_small/0060185-241126133413365.parquet\")\n",
    "df2 = pd.read_parquet(\"/home/george/codes/gbifxdl/data/classif/lepi_small/0060185-241126133413365_v2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[~df1.apply(tuple,1).isin(df2.apply(tuple,1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.equals(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df1), len(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[df1['continent']=='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['url_hash'].tail(), df2['filename'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.img_hash.duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df.status == 'downloading_failed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to stream the DWCA file to avoid loading it into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occurrences_path = Path(\"/mnt/c/Users/au761367/OneDrive - Aarhus universitet/Codes/python/gbifxdl/data/classif/mini/0013397-241007104925546.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with DwCAReader(occurrences_path) as dwca:\n",
    "    print(dwca.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwca = DwCAReader(occurrences_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = dwca.get_corerow_by_position(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SFTPClient connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from paramiko import SSHClient, AutoAddPolicy, SFTPClient, Transport, RSAKey\n",
    "\n",
    "class SFTPHandler:\n",
    "    def __init__(self, host, port, username, rsa_key_path=None, working_dir=\"/\"):\n",
    "        \"\"\"\n",
    "        Initialize the SFTPHandler with RSA key authentication.\n",
    "        :param host: SFTP server hostname\n",
    "        :param port: SFTP server port\n",
    "        :param username: Username for authentication\n",
    "        :param rsa_key_path: Path to the RSA private key file (optional)\n",
    "        :param rsa_key_str: RSA private key as a string (optional)\n",
    "        \"\"\"\n",
    "        self.transport = Transport((host, port))\n",
    "        \n",
    "        # Load RSA Key\n",
    "        if rsa_key_path:\n",
    "            rsa_key = RSAKey.from_private_key_file(rsa_key_path)\n",
    "        else:\n",
    "            raise ValueError(\"Either 'rsa_key_path' or 'rsa_key_str' must be provided.\")\n",
    "        \n",
    "        # Connect with RSA Key\n",
    "        self.transport.connect(username=username, pkey=rsa_key)\n",
    "        self.sftp = SFTPClient.from_transport(self.transport)\n",
    "        self.create_folder(working_dir)\n",
    "        self.sftp.chdir(working_dir)\n",
    "    \n",
    "    def create_folder(self, folder):\n",
    "        try:\n",
    "            self.sftp.mkdir(folder)\n",
    "        except IOError:\n",
    "            pass  # Folder likely exists\n",
    "\n",
    "    def upload_file(self, folder, filename, file_data):\n",
    "        self.create_folder(folder)\n",
    "        remote_path = os.path.join(folder, filename)\n",
    "        self.sftp.putfo(file_data, remote_path)\n",
    "        # with self.sftp.open(remote_path, \"wb\") as f:\n",
    "        #     f.write(file_data)\n",
    "\n",
    "    def close(self):\n",
    "        self.sftp.close()\n",
    "        self.transport.close()\n",
    "\n",
    "sftp = SFTPHandler(\n",
    "    host=\"io.erda.au.dk\",\n",
    "    port=2222,\n",
    "    working_dir=\"datasets/test4\",\n",
    "    username=\"gmo@ecos.au.dk\",\n",
    "    rsa_key_path=\"/mnt/c/Users/au761367/.ssh/id_rsa\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"/mnt/c/Users/au761367/OneDrive - Aarhus universitet/Codes/python/gbifxdl/data/classif/mini/images/1011881/1c41c4a0ed1dc2c62fda5f30f3844bddb0f66ed5.jpeg\"\n",
    "img=Image.open(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(img_path, 'br') as img:\n",
    "    sftp.upload_file(\"/\", \"1c41c4a0ed1dc2c62fda5f30f3844bddb0f66ed5.jpeg\", img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SFTPClient connection with async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_client() -> None:\n",
    "    async with asyncssh.connect(\n",
    "        host=\"io.erda.au.dk\",\n",
    "        port=2222,\n",
    "        username=\"gmo@ecos.au.dk\",\n",
    "        client_keys=[\"/mnt/c/Users/au761367/.ssh/id_rsa\"]\n",
    "        ) as conn:\n",
    "        async with conn.start_sftp_client() as sftp:\n",
    "            await sftp.get('datasets/test3/1011881/1c41c4a0ed1dc2c62fda5f30f3844bddb0f66ed5.jpeg')\n",
    "\n",
    "try:\n",
    "    asyncio.run(run_client())\n",
    "except (OSError, asyncssh.Error) as exc:\n",
    "    sys.exit('SFTP operation failed: ' + str(exc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test of TypedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class test(TypedDict):\n",
    "    coucou: str = \"h\"\n",
    "    num: int = None\n",
    "\n",
    "def f(coucou, num):\n",
    "    print(f\"{coucou*num}\")\n",
    "\n",
    "t = test({\"coucou\":\"hello \", \"num\":5})\n",
    "# print(**t)\n",
    "f(**t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(coucou=\"hello\", num=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check metadata quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet(path=\"/home/george/codes/gbifxdl/data/classif/mini/0013397-241007104925546_processing_metadata.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_path=\"/mnt/c/Users/au761367/OneDrive - Aarhus universitet/Codes/python/gbifxdl/data/classif/mini/0013397-241007104925546.parquet\"\n",
    "batch_size = 10\n",
    "parquet_iter_for_merge = pq.ParquetFile(parquet_path).iter_batches(batch_size=batch_size)\n",
    "original_table = pa.Table.from_batches([next(parquet_iter_for_merge)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to use dwca.pd_read "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is extremely memory expensive\n",
    "# with DwCAReader(\"/home/george/codes/gbifxdl/data/classif/lepi/0061420-241126133413365.zip\") as dwca:\n",
    "#     media_df = dwca.pd_read(\"multimedia.txt\", parse_dates=True, on_bad_lines=\"skip\")\n",
    "#     occ_df = dwca.pd_read(\"occurrence.txt\", parse_dates=True, on_bad_lines=\"skip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with DwCAReader(\"/home/george/codes/gbifxdl/data/classif/lepi_small/0060185-241126133413365.zip\") as dwca:\n",
    "    media_df = dwca.pd_read(\"multimedia.txt\", parse_dates=True, on_bad_lines=\"skip\", chunksize=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk = next(iter(media_df))\n",
    "chunk.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to open large parquet file in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"/home/george/codes/gbifxdl/data/classif/lepi/0061420-241126133413365_sampled.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use dask to open it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_parquet(\"/home/george/codes/gbifxdl/data/classif/lepi/0061420-241126133413365.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute speciesKey distribution\n",
    "species_distribution = df[\"speciesKey\"].value_counts().compute()\n",
    "\n",
    "# Plot the distribution\n",
    "species_distribution.plot(kind=\"bar\", figsize=(12, 6))\n",
    "plt.xlabel(\"speciesKey\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"SpeciesKey Distribution\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to sample 500 rows per speciesKey\n",
    "def sample_species(group):\n",
    "    # Ensure sampling is done correctly in Pandas\n",
    "    return group.sample(n=min(len(group), 500), random_state=42)\n",
    "\n",
    "# Group by speciesKey and sample\n",
    "sampled_df = df.groupby(\"speciesKey\").apply(\n",
    "    sample_species, meta=df\n",
    ")\n",
    "\n",
    "# Persist the result (optional, to optimize memory usage)\n",
    "sampled_df = sampled_df.persist()\n",
    "\n",
    "# Save the sampled rows to a new Parquet file\n",
    "output_path = \"sampled_species.parquet\"\n",
    "sampled_df.to_parquet(output_path, write_index=False)\n",
    "\n",
    "print(f\"Sampled data saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
