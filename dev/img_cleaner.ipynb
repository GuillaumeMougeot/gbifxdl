{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from os import listdir\n",
    "from os.path import isdir, join\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from mini_trainer import predict\n",
    "from mini_trainer.builders import BaseBuilder\n",
    "from mini_trainer.utils.io import ImageLoader\n",
    "from torch.utils.data import DataLoader\n",
    "from mini_trainer.utils.logging import BaseResultCollector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir=\"/home/george/codes/gbifxdl/data/cleaner/images\"\n",
    "csv_path=\"/home/george/codes/gbifxdl/data/cleaner/images/fold.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List files\n",
    "filenames = {d:[] for d in sorted(listdir(data_dir))}\n",
    "for d in listdir(data_dir):\n",
    "    sub_dir = join(data_dir, d)\n",
    "    if isdir(sub_dir):\n",
    "        for f in listdir(sub_dir):\n",
    "            filenames[d] += [f]\n",
    "pprint(filenames[\"1\"][:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data distribution.\n",
    "\n",
    "x = list(filenames.keys())\n",
    "height = [len(v) for v in filenames.values()]\n",
    "\n",
    "print(x, height)\n",
    "\n",
    "plt.bar(x, height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imbalance degree.\n",
    "\n",
    "height = [len(v) for v in filenames.values()]\n",
    "for i in range(len(height)):\n",
    "    print(f\"Imbalance degree: {height[i]/max(height)*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into train/val/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hold_out(X, y, split_rate, seed=42):\n",
    "    \"\"\"\n",
    "    Splits the dataset into training and test samples in a balanced fashion across classes.\n",
    "    \n",
    "    Args:\n",
    "        X (list): List of inputs (e.g., filenames).\n",
    "        y (list): List of labels corresponding to X.\n",
    "        split_rate (float): Fraction of samples to assign as test per class.\n",
    "        \n",
    "    Returns:\n",
    "        list: A binary list of length len(y), where 1 indicates a test (hold-out) sample.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    n = len(y)\n",
    "    hold_out_mask = [0] * n\n",
    "    # Group indices by class label.\n",
    "    class_indices = defaultdict(list)\n",
    "    for i, label in enumerate(y):\n",
    "        class_indices[label].append(i)\n",
    "    \n",
    "    # For each class, randomly sample indices for the test set.\n",
    "    for label, indices in class_indices.items():\n",
    "        n_samples = len(indices)\n",
    "        n_test = int(round(n_samples * split_rate))\n",
    "        test_indices = random.sample(indices, n_test)\n",
    "        for idx in test_indices:\n",
    "            hold_out_mask[idx] = 1\n",
    "            \n",
    "    return hold_out_mask\n",
    "\n",
    "def stratified_kfold(X, y, n_splits, hold_out_mask=None, seed=42):\n",
    "    \"\"\"\n",
    "    Performs stratified k-fold assignment on the dataset.\n",
    "    \n",
    "    For each class, training samples (or all samples if hold_out_mask is None)\n",
    "    are randomly shuffled and assigned fold numbers (0 to n_splits-1) in a round-robin fashion.\n",
    "    If hold_out_mask is provided, samples with hold_out==1 are skipped (their fold is left as -1).\n",
    "    \n",
    "    Args:\n",
    "        X (list): List of inputs.\n",
    "        y (list): List of labels.\n",
    "        n_splits (int): Number of folds.\n",
    "        hold_out_mask (list, optional): List of 0s and 1s indicating training/test. Defaults to None.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of fold assignments (length equal to len(y)). For training samples, the fold is an integer\n",
    "              in the range [0, n_splits-1]. Test samples get a fold assignment of -1.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    n = len(y)\n",
    "    folds = [-1] * n  # default: -1 for test samples\n",
    "    # Group training indices by class.\n",
    "    class_indices = defaultdict(list)\n",
    "    for i, label in enumerate(y):\n",
    "        if hold_out_mask is not None:\n",
    "            if hold_out_mask[i] == 0:\n",
    "                class_indices[label].append(i)\n",
    "        else:\n",
    "            class_indices[label].append(i)\n",
    "    \n",
    "    # Assign fold numbers in a round-robin fashion within each class.\n",
    "    for label, indices in class_indices.items():\n",
    "        random.shuffle(indices)\n",
    "        for j, idx in enumerate(indices):\n",
    "            folds[idx] = j % n_splits\n",
    "    return folds\n",
    "\n",
    "def save_stratified_kfold_csv(X, y, split_rate, n_splits, csv_filename):\n",
    "    \"\"\"\n",
    "    Combines hold-out splitting and stratified k-folding, then writes the results to a CSV file.\n",
    "    \n",
    "    The CSV will have three columns:\n",
    "      - filename (taken from X)\n",
    "      - hold_out (0 for training, 1 for test)\n",
    "      - fold (fold number for training samples; -1 for test samples)\n",
    "    \n",
    "    Args:\n",
    "        X (list): List of filenames (or input identifiers).\n",
    "        y (list): List of labels.\n",
    "        split_rate (float): Fraction of samples per class to mark as test.\n",
    "        n_splits (int): Number of folds for stratified k-folding.\n",
    "        csv_filename (str): Path to the CSV file to be saved.\n",
    "    \"\"\"\n",
    "    # First, compute the hold-out mask.\n",
    "    hold_out_mask = hold_out(X, y, split_rate)\n",
    "    # Next, compute stratified k-fold assignments (training samples get a fold number, test samples get -1).\n",
    "    folds = stratified_kfold(X, y, n_splits, hold_out_mask)\n",
    "    \n",
    "    # Write the results to a CSV file.\n",
    "    with open(csv_filename, mode='w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['filename', 'fold'])\n",
    "        for filename, fold in zip(X, folds):\n",
    "            writer.writerow([filename, fold])\n",
    "\n",
    "def split_dataset(root_dir, split_rate, n_splits, csv_filename, abs_path=False):\n",
    "    \"\"\"\n",
    "    Processes the dataset stored in a folder structure where each subfolder represents a class.\n",
    "    \n",
    "    It collects image file paths and their corresponding class labels (the subfolder names), \n",
    "    performs a balanced hold-out split and stratified k-folding, and saves the splits into a CSV file.\n",
    "    \n",
    "    The resulting CSV contains:\n",
    "      - filename: full path to the image file\n",
    "      - hold_out: 1 if the image is assigned as a test sample, 0 otherwise\n",
    "      - fold: the fold number for training samples (0 to n_splits-1) or -1 for test samples.\n",
    "    \n",
    "    Args:\n",
    "        root_dir (str): Path to the root directory of the dataset.\n",
    "        split_rate (float): Fraction of images per class to mark as test.\n",
    "        n_splits (int): Number of folds for stratified k-folding.\n",
    "        csv_filename (str): Path to the CSV file to be saved.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    # Each subdirectory in root_dir is assumed to be a class.\n",
    "    for class_name in os.listdir(root_dir):\n",
    "        class_dir = os.path.join(root_dir, class_name)\n",
    "        if os.path.isdir(class_dir):\n",
    "            # List all files in the class directory.\n",
    "            for file in os.listdir(class_dir):\n",
    "                file_path = os.path.join(class_dir, file)\n",
    "                if os.path.isfile(file_path):\n",
    "                    if abs_path:\n",
    "                        X.append(file_path)\n",
    "                    else:\n",
    "                        X.append(file)\n",
    "                    y.append(class_name)\n",
    "                    \n",
    "    # Save the stratified k-fold and hold-out splits to CSV.\n",
    "    save_stratified_kfold_csv(X, y, split_rate, n_splits, csv_filename)\n",
    "    print(f\"CSV with stratified k-fold splits saved to {csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset(\n",
    "    root_dir=data_dir,\n",
    "    split_rate=0.2,\n",
    "    n_splits=5,\n",
    "    csv_filename=csv_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First try. Training on the imbalance dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second try. Use Asger's mini trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=os.listdir(\"/home/george/codes/gbifxdl/data/lepi_small/images\")\n",
    "al=[os.listdir(os.path.join(\"/home/george/codes/gbifxdl/data/lepi_small/images\",e)) for e in l]\n",
    "lal=[len(e) for e in al]\n",
    "l[np.argmax(lal)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = predict.cli(description=\"Mini trainer.\",\n",
    "                     models=\"efficientnet_v2_s\",\n",
    "                     class_index=\"/home/george/codes/gbifxdl/data/cleaner/models/class_index.json\",\n",
    "                     weights=\"/home/george/codes/gbifxdl/data/cleaner/models/efficientnet_v2_s_full_e15.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict.main(\n",
    "    input=\"/home/george/codes/gbifxdl/data/cleaner/images/5\",\n",
    "    output=\"/home/george/codes/gbifxdl/data/cleaner/tests\",\n",
    "    class_index=\"/home/george/codes/gbifxdl/data/cleaner/models/class_index.json\",\n",
    "    model_builder_kwargs={\n",
    "        \"model_type\":\"efficientnet_v2_s\",\n",
    "        \"weights\":\"/home/george/codes/gbifxdl/data/cleaner/models/efficientnet_v2_s_full_e15.pt\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = \"/home/george/codes/gbifxdl/data/cleaner/images\"\n",
    "all_folders = [os.path.join(main_path,e) for e in os.listdir(main_path)]\n",
    "for folder in all_folders:\n",
    "    res=predict.main(\n",
    "        input=folder,\n",
    "        output=\"/home/george/codes/gbifxdl/data/cleaner/tests\",\n",
    "        class_index=\"/home/george/codes/gbifxdl/data/cleaner/models/class_index.json\",\n",
    "        model_builder_kwargs={\n",
    "            \"model_type\":\"efficientnet_v2_s\",\n",
    "            \"weights\":\"/home/george/codes/gbifxdl/data/cleaner/models/efficientnet_v2_s_full_e15.pt\"\n",
    "        }\n",
    "    )\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewrite predict.main so to take as input:\n",
    "- image folder\n",
    "- metadata file\n",
    "\n",
    "and to output:\n",
    "- either an edited metadata file.\n",
    "- or an edited folder (non-1 classes removed).\n",
    "\n",
    "There could be a loop that takes a batch of filenames/paths and that output the \n",
    "model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Postprocessing:\n",
    "    model_list = ['efficientnet_v2_s']\n",
    "    def __init__(self,\n",
    "                 class_index:str,\n",
    "                 batch_size:int,\n",
    "                 workers:int,\n",
    "                 device:torch.device,\n",
    "                 model_type:str,\n",
    "                 weights_path:str,\n",
    "                 ):\n",
    "        f\"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        class_index : str\n",
    "            Path to the class index.\n",
    "        batch_size : int\n",
    "        workers : int\n",
    "        device : torch.device\n",
    "            Device where the model and the data batches will be placed.\n",
    "        model_type : str\n",
    "            One of {self.model_list}.\n",
    "        weights_path : str\n",
    "            Path toward the model weights.        \n",
    "        \"\"\"\n",
    "        assert model_type in self.model_list, f\"Model should be one of {self.model_list}\"\n",
    "\n",
    "        builder = BaseBuilder()\n",
    "\n",
    "        extra_model_kwargs, self.extra_dataloader_kwargs = builder.spec_model_dataloader(\n",
    "            path=class_index, \n",
    "            dir=None,\n",
    "        )\n",
    "\n",
    "        self.dtype : torch.dtype = getattr(torch, \"float16\")\n",
    "        self.device : torch.device = torch.device(device)\n",
    "        self.model, model_preprocess = builder.build_model( \n",
    "            device=self.device, \n",
    "            dtype=self.dtype, \n",
    "            model_type = model_type,\n",
    "            weights = weights_path,\n",
    "            **extra_model_kwargs\n",
    "        )\n",
    "        self.model.eval()\n",
    "\n",
    "        self.image_loader = ImageLoader(\n",
    "            model_preprocess,\n",
    "            self.dtype,\n",
    "            torch.device(\"cpu\")\n",
    "        )\n",
    "        self.batch_size = batch_size\n",
    "        self.workers = workers\n",
    "\n",
    "    def predict(self, paths):\n",
    "        \"\"\"Predict from list of paths.\"\"\"\n",
    "        ds = self.image_loader(paths)\n",
    "        dl = DataLoader(\n",
    "            ds,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.workers,\n",
    "            pin_memory=True,\n",
    "            pin_memory_device=self.device.type,\n",
    "            shuffle=False,\n",
    "            drop_last=False\n",
    "        )\n",
    "\n",
    "        results=BaseResultCollector(\n",
    "            training_format=False,\n",
    "            verbose=False,\n",
    "            **self.extra_dataloader_kwargs)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_i, batch in tqdm(enumerate(dl), desc=\"Running inference...\", total=np.ceil(len(ds) / self.batch_size), leave=True):\n",
    "                i = batch_i * self.batch_size\n",
    "                if len(batch.shape) == 3:\n",
    "                    batch = batch.unsqueeze(0)\n",
    "                with torch.autocast(device_type=self.device.type, dtype=self.dtype):\n",
    "                    prediction = self.model(batch.to(self.device))\n",
    "                results.collect(\n",
    "                    paths = paths[i:(i+len(batch))],\n",
    "                    predictions = prediction\n",
    "                )\n",
    "\n",
    "        return results.data\n",
    "    \n",
    "    def process(self, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp=Postprocessing(\n",
    "    class_index=\"/home/george/codes/gbifxdl/data/cleaner/models/class_index.json\",\n",
    "    batch_size=16,\n",
    "    workers=6,\n",
    "    # device=torch.device(\"cuda:0\"),\n",
    "    device=\"cuda\",\n",
    "    model_type=\"efficientnet_v2_s\",\n",
    "    weights_path=\"/home/george/codes/gbifxdl/data/cleaner/models/efficientnet_v2_s_full_e15.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=\"/home/george/codes/gbifxdl/data/insectnet/images/8049830\"\n",
    "l=os.listdir(f)\n",
    "al=[os.path.join(f,e) for e in l]\n",
    "\n",
    "preds=pp.predict(al)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=[i for i,p in enumerate(preds[\"preds\"]) if p!='1']\n",
    "non1 = {k:[p[i] for i in idx] for k,p in preds.items()}\n",
    "\n",
    "paths, ps, cnfs = non1[\"paths\"], non1[\"preds\"], non1[\"confs\"]\n",
    "\n",
    "for i,pths in enumerate(paths):\n",
    "    plt.imshow(Image.open(pths))\n",
    "    plt.title(f\"Pred {ps[i]}, Conf {cnfs[i]}\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
